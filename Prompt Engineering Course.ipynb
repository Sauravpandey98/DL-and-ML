{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ad528c",
   "metadata": {},
   "source": [
    "# This tutorial is based on Deeplearning prompt engineering crash course\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f514452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca559fe",
   "metadata": {},
   "source": [
    "# Part 1: Important Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c42f4",
   "metadata": {},
   "source": [
    "## First Principle\n",
    "\n",
    "First prinnciple of writing good prompts is to write is clear and precise.It does not matter how long the prompt is if it able to clearly specify the task.Following tactics can be followed to write such prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e55c1d",
   "metadata": {},
   "source": [
    "### Use Delimeter and punctuation to separate different sections of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\ \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "#In this example I tried by removing some delimeters and making the prompt less structured.\n",
    "#But it does not seemed to have major effect.But still it can be assumed \n",
    "#that better answer will be given for the prompt which follows this tactics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56ef1d",
   "metadata": {},
   "source": [
    "### this tactic is development based.As we can ask the ChatGPT to also return the output in particular format.This feature provides us ease of processing a standard format output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b94157",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Generate a list of three made-up book titles along \\ \n",
    "with their authors and genres. \n",
    "Provide them in JSON format with the following keys: \n",
    "book_id, title, author, genre.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02558639",
   "metadata": {},
   "source": [
    "### We can also ask the ChatGPT to provide condition based answers.We can multiple conditions and thier responses in the promp.One use of this type of feature is responding the user on the basis of emotion predicted in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24084c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 =\n",
    "f\"\"\"\n",
    "Saurav is smart and intelligent.\n",
    "\"\"\" \n",
    "\n",
    "prompt =\n",
    "f\"\"\"\n",
    "You will be provided with text delimited by triple backticks. \n",
    "\n",
    "If the text contains only good things about Saurav.Then write the following response: \\\n",
    "\\\"Thanks,You are good guy. \\\"\n",
    "\n",
    "-------------------------------------\n",
    "If the text indicate that the writer is jealous from Saurav.Then write the following response: \\\n",
    "\\\"It seems that you are jealous of Saurav. \\\"\n",
    "\n",
    "--------------------------------------\n",
    "If the text contains at least one thing bad about Saurav and it does not indicate jealousy.Then write the following response: \\\n",
    "\\\"You are `emotion` of Saurav.\\\".Here `emotion' is a place holder for the emotion depicted in the text.\n",
    "\n",
    "---------------------------------------\n",
    "The text is: \\\n",
    "```{text_2}```\n",
    "\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 2:\")\n",
    "print(response)\n",
    "\n",
    "#it was a funny way of checking this feature.* - *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72f0c8",
   "metadata": {},
   "source": [
    "### Few Shot Prompting.This feature is based on making the model understand the output structure and then reply to the input.There are some use cases that I can think of this feature:\n",
    "\n",
    "* Solving Puzzles\n",
    "* Restructuring Structured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4710999",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be given some example conatining the question and thier respective answer. \\\n",
    "Through these examples, you need to understand the reason behind each answer. \\\n",
    "Each answer will be based upon same reasoning. \\\n",
    "Your task is to give the answer of a question using the same reasoning that was appliied while solving the example questions.\n",
    "\n",
    " ---------------------\n",
    " Examples:\n",
    " 1. Question: Apple+Mango\n",
    "    Answer: Orange\n",
    " 2. Question: Blueberry+Pineberry\n",
    "    Answer: Skyblue\n",
    " 3. Question: Blueberry+Mango\n",
    "    Answer:   Green\n",
    "\n",
    " Question to be answered:\n",
    " Question: Apple+Green Grapes\n",
    " Answer: ?\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "##In this example model is not able to understand the reasoning clearly.The answer given by Model is \"Red\".But the actual answer is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef3191",
   "metadata": {},
   "source": [
    "## Principal 2: Give the model time to think"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c676c98",
   "metadata": {},
   "source": [
    "### In case if the problem is little complex to solve,then we can change the prompt using following tactics.These tactics increase the amount of computation but also helps in increasing accuracy:\n",
    "\n",
    "* Specify the steps that model should follow to get the response\n",
    "\n",
    "* Ask the model to work out its own solution first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433bdb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \\\n",
    " help working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\ \n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "#the answer in this case was wrong,hence we will follow the second point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a670aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution \\\n",
    "is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem. \n",
    "- Then compare your solution to the student's solution \\ \n",
    "and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until \n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I'm building a solar power installation and I need help \\\n",
    "working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \\\n",
    "as a function of the number of square feet.\n",
    "``` \n",
    "Student's solution:\n",
    "```\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "Actual solution:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "#in this case answer came out be correct.Let's try this method in our own problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c706c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lets try some points from above prompt\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\n",
    "Your task is to give the solution of the question:\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the questions whoose answer is given\n",
    "- Then compare your solutions to the given answers\n",
    "and evaluate if your solutions is same as given answers. \n",
    "Don't decide on the solution of unanswered question until all the answers of the questions whoose answer is given \\\n",
    "matched to your answers.\n",
    "\n",
    "-----------------------\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Answer:\n",
    "```\n",
    "answer here\n",
    "```\n",
    "\n",
    " ---------------------\n",
    "Question\n",
    "```\n",
    "You need to give correct answer of the question where answer is written as `?`.All the answer is based on some specific reasoning.\\\n",
    "You can deduce that reasoning from the questions for which answers are available.And aplly same reasoning to the other questions that needed to be answered.\n",
    " 1. Question: Apple+Mango\n",
    "    Answer: Orange\n",
    " 2. Question: Blueberry+Pineberry\n",
    "    Answer: Skyblue\n",
    " 3. Question: Blueberry+Mango\n",
    "    Answer:   Green\n",
    " 4.  Question: Apple+Green Grapes\n",
    "     Answer: ?\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "##Here also model failed.Hmm,do now know where the problem is.Might try another variation later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9356f7",
   "metadata": {},
   "source": [
    "## Model Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7294c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some the model can create information that is not actually present\n",
    "prompt = f\"\"\"\n",
    "Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "#here Boie is real but there is not product like AeroGlide UltraSlim Samrt Toothbrush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is one way to reduce this problem,\n",
    "#is to ask the model to find the relevant information and then answer based on that relevant information\n",
    "prompt = f\"\"\"\n",
    "\n",
    "```\n",
    "Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie.\n",
    "```\n",
    "\n",
    "While answering this question firstly find relevant information.\\\n",
    "Then answer the question based on the relevant information\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "#here the answer comes out to be correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b264afc",
   "metadata": {},
   "source": [
    "## Let's explore the summary building capability of ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e22a6c",
   "metadata": {},
   "source": [
    "### let's build a reviewer reader for a ecommerce site that can have following features:\n",
    "* It should be able to summarise the review based on the following context filters:\n",
    "    * Product\n",
    "    * Pricing\n",
    "    * Shipping\n",
    "* It should be able to summarise the review to the number of words inputted by user.\n",
    "* In the summary it should be able to put more focus on a particular context.The context can be the following and will be inputted by user\n",
    "    * Product\n",
    "    * Pricing\n",
    "    * Shipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1152f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some custom data types\n",
    "from enum import Enum\n",
    "from typing import Any\n",
    "\n",
    "# for context\n",
    "class Context(Enum):\n",
    "    PRODUCT=\"PRODUCT\"\n",
    "    PRICING=\"PRICING\"\n",
    "    SHIPPING=\"SHIPPING\"\n",
    "\n",
    "# a data type that is used to limiting the number of words \n",
    "class LimitNumber:\n",
    "    def __init__(self, value):\n",
    "        if not isinstance(value, int) or value <= 0:\n",
    "            raise ValueError(\"Value must be a positive integer.\")\n",
    "        self.value = value\n",
    "        \n",
    "class ContextReviewPrompts():\n",
    "    \n",
    "    PRODUCT_REVIEW_PROMPT=\"\"\"to give feedback to the \\\n",
    "    product deparmtment, responsible for overseeing merchandise in online \\\n",
    "    business operations and strategies.  \\\n",
    "\n",
    "    Summarize the review below, delimited by triple \\\n",
    "    backticks,by focusing on any aspects \\\n",
    "    that are relevant to the product quality.\"\"\"\n",
    "    \n",
    "    SHIPPING_REVIEW_PROMPT=\"\"\"to give feedback to the \\\n",
    "    shipping deparmtment. \\\n",
    "\n",
    "    Summarize the review below, delimited by triple \\\n",
    "    backticks, by focusing on any aspects \\\n",
    "    that are relevant to the shipping and delivery\"\"\"\n",
    "    \n",
    "    PRICING_REVIEW_PROMPT=\"\"\"\n",
    "    to give feedback to the \\\n",
    "    pricing deparmtment, responsible for determining the \\\n",
    "    price of the product.  \\\n",
    "\n",
    "    Summarize the review below, delimited by triple \\\n",
    "    backticks, by focusing on any aspects \\\n",
    "    that are relevant to the price and perceived value.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, context: Context) -> Any:\n",
    "        \n",
    "        if context == Context.PRODUCT:\n",
    "            return self.PRODUCT_REVIEW_PROMPT\n",
    "        \n",
    "        if context == Context.PRICING:\n",
    "            return self.SHIPPING_REVIEW_PROMPT\n",
    "        \n",
    "        if context == Context.SHIPPING:\n",
    "            return self.SHIPPING_REVIEW_PROMPT  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea62521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to give feedback to the     shipping deparmtment. \\n    Summarize the review below, delimited by triple     backticks, by focusing on any aspects     that are relevant to the shipping and delivery'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_review_prompts=ContextReviewPrompts()\n",
    "context_review_prompts(Context.PRICING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
