{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions still needs to be answered:\n",
    "1. Why does we substract gradient from the weights?\n",
    "[Answer](https://towardsdatascience.com/wondering-why-do-you-subtract-gradient-in-a-gradient-descent-algorithm-9b5aabdf8150#:~:text=While%20in%20gradient%20descent%20algorithm,in%20the%20gradient%20descent%20algorithm)\n",
    "2. How does autograd works under the hood in pytorch?\n",
    "3. Why we need a vector to do calculate the gradient of a vector output?\n",
    "4. What is gradient accumulation and how does it affects the taining process?\n",
    "[Answer](https://stackoverflow.com/questions/62067400/understanding-accumulated-gradients-in-pytorch)\n",
    "5. Why we add a bias to the linear model formula?\n",
    "Ans. So we add bias to encounter the different variation in the function to be approximated.\n",
    "Let consider two variations:\n",
    "  * Linear Approximation: \n",
    "  In case of linear approximation,if we do not add bias to the equation then it will always approximate the equation that passed through origin.\n",
    "  * Non linear approximation (Neural Networks): \n",
    "\n",
    "    In case of neural network,a single layer can be represented by:\n",
    "\n",
    "    y=f(WX+b)\n",
    "\n",
    "    where f is non linear activation function,W is weight vector,X is input vector and b is bias.\n",
    "    \n",
    "    Now let's consider two types of activation functions:\n",
    "    \n",
    "    * Odd Activation functions(f(-x)=-f(x)).Function that passes through origin:\n",
    "\n",
    "    In this case,if x is zero and b is zero,then f(0)=0.Hence we will never be able to approximate where y(0) is not equal to zero.\n",
    "\n",
    "    * Even activation functions:\n",
    "\n",
    "    In such cases,the activation functions can approximate all kinds of functions.But with biases, first of all it reduces the chances of zero gradients(vanishing gradients) as some of the activation functions(tanh,ReLU) approaches to zero when input approaches zero.Also this helps the model to approximate more complex functions.\n",
    "    [REF](https://www.turing.com/kb/necessity-of-bias-in-neural-networks)\n",
    "6. what is difference between Binary Cross Entropy & Cross Entropy and Mean Squared Error?\n",
    "Ans.\n",
    "7. All types of activation functions and what are thier advantages?\n",
    "Ans. https://medium.com/@snaily16/what-why-and-which-activation-functions-b2bf748c0441\n",
    "8. Building blocks of CNN?\n",
    "Ans: [ANSWER](https://mylearningsinaiml.wordpress.com/deep-learning/dl4cv/cnn-building-blocks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "* Tensors are analogous to array and can represent scalar,vector or matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating tensors\n",
    "a=torch.rand((2,3),dtype=torch.float32)\n",
    "b=torch.ones((2,3)) #generate tensor with one values\n",
    "c=torch.empty((2,3)) # generate a tensor with zero values\n",
    "d=torch.zeros((2,3)) # generate a tensor with zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing operation on tensors\n",
    "added=a+b #operating with tensors\n",
    "added1=a+1 #operating with scalars\n",
    "mul=a*b\n",
    "\n",
    "#inplace operations\n",
    "a.add(1) # will return a new tensor\n",
    "a.add_(1) #in pytorch any operation suffixed by underscore is inplace operation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor to numpy array and vice versa\n",
    "t=torch.rand((2,2))\n",
    "t_nump=t.numpy()\n",
    "\n",
    "print(\"tensor is\",t)\n",
    "print(\"numpy array is\",t_nump)\n",
    "print(\"numpy array type\",type(t_nump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#please note that both t and t_numpy will refer to same memory\n",
    "t_nump[0][0]=1\n",
    "\n",
    "print(\"tensor is\",t)\n",
    "print(\"numpy array is\",t_nump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new tensor through reshaping\n",
    "t_new=t.view((-1,4)) #-1 is just a place holder,the pytorch will y itself decide the first dimension on the basis of value of other dimnesion\n",
    "\n",
    "t_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing a subpart of tensor\n",
    "t[0,:] #first row,all columns\n",
    "t[:,0] #all rows zeroeht columns\n",
    "t[:,-1] #all rows last column\n",
    "t[0:2:,-1:] #last column,first and last row\n",
    "t[0,1] #a single element,here dtype will be tensor\n",
    "t[0,1].item() #a single element,here dtype will be scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGrad: \n",
    "It is inbuilt library in pytorch that handles the creation of computational graph needed for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a computational graph\n",
    "x=torch.rand((4,4),requires_grad=True) #setting requires_grad True will include the variable x in computation graph\n",
    "y=x**2+2 #all the variables related to x will be also included in this gradient graph\n",
    "z=y.mean()\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calculate gradient\n",
    "z.backward() #will calculate the gradient of z with respect to every variable in the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad) #will have grad information as it contain a leaf node\n",
    "print(y.grad) #no grad information as it is not a leaf node\n",
    "print(z.grad) #no grad information as it is not a leaf node\n",
    "\n",
    "#the definition of leaf node is as follows:\n",
    "\n",
    "# In general a leaf node is a node which has no childs,so in computational graph a leaf node is a node which has no incoming edges,so in pytorch a leaf node is a tensor which has requires_grad=False.\n",
    "# All Tensors that have requires_grad which is False will be leaf Tensors by convention.\n",
    "\n",
    "# For Tensors that have requires_grad which is True , they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.\n",
    "\n",
    "# Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad() ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics of autograd\n",
    "#gradient at a \n",
    "x.grad=None\n",
    "x=torch.tensor(2,dtype=torch.float32,requires_grad=True) #input of graph\n",
    "z=x*2  # z node has 1 input,so autograd will automatically create a graph that includes derivative of z with respect x(dz/dx)\n",
    "y=z*2  #similariy for y (dy/dz)\n",
    "y.backward() #will propagate the create backward graph\n",
    "\n",
    "x.grad #dy/dx=(dy/dz)*(dz/dx)=(2*z)*(2*x)=(2*4)*(2*2)=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "Backpropagation is a technique used in neural networks to calculate the gradients of the loss function with respect to the weights of the network. It is a key component of training neural networks through gradient descent optimization.Please read the uploaded autodifferentiation files for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models. It iteratively adjusts the model parameters in the direction of steepest descent of the loss function. Backpropagation is a technique used to calculate the gradients of the loss function with respect to the model parameters.\n",
    "\n",
    "## General steps to be followed while training a model:\n",
    "1. Define the model along with loss function.\n",
    "2. Initialse the model with random weights.\n",
    "3. With each batch of input calculate loss(Loss Calculation)\n",
    "4. Compute Gradients of parameters wrt. to loss (Gradient Computation)\n",
    "5. Update parameters (Gradient Update).\n",
    "Repeat the steps between 3 to 5 till the given total iterations.\n",
    "\n",
    "By iteratively updating the model parameters based on the gradients calculated through backpropagation, gradient descent aims to find the optimal set of parameters that minimize the loss function and improve the model's performance.\n",
    "\n",
    "### Let's implement gradient descent for a simple linear regression model.To make this more interestin,we will do it in following steps:\n",
    "1. Implement it using python numpy\n",
    "2. Implement it using pytorch\n",
    "    1. Using only gradient computation feature\n",
    "    2. Using gradient computation and loss caluclation feature\n",
    "    3. Using gradient update feature\n",
    "    4. Using model creation feature\n",
    "\n",
    "### Function to optimise\n",
    "    y=w*x\n",
    "     where y is output and x is input.\n",
    "     The final optimized function should be ~ 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([1,2,3,4])\n",
    "y_gt=np.array([2,4,6,8])\n",
    "\n",
    "w=np.random.rand(1).astype(np.float32)\n",
    "learning_rate=0.01\n",
    "\n",
    "#mode prediction\n",
    "def forward(x,w):\n",
    "    return x*w\n",
    "\n",
    "#loss function\n",
    "#MSE= (1/n)*sum((w*x-y_gt)^2)\n",
    "def loss(y_gt,y_pred):\n",
    "    #L=(y_gt-y_pred)^2\n",
    "    return np.mean((y_gt-y_pred)**2)\n",
    "\n",
    "#gradient computation\n",
    "# dL/dw=(1/n)*sum(2*(y_pred-y_gt)*x)\n",
    "def gradient(x,y_gt,y_pred):\n",
    "    return np.mean(2*(y_pred-y_gt)*x)\n",
    "\n",
    "def paramter_update(w,grad,lr):\n",
    "    return w-lr*grad\n",
    "\n",
    "for i in range(100):\n",
    "    y_pred=forward(x,w)\n",
    "    l=loss(y_gt,y_pred)\n",
    "    grad=gradient(x,y_gt,y_pred)\n",
    "    w=paramter_update(w,grad,learning_rate)\n",
    "    print(\"#########iteration##########\",i)\n",
    "    print(\"loss is\",l)\n",
    "    print(\"gradient is\",grad)\n",
    "    print(\"w is\",w)\n",
    "    print(\"predicted y is\",y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pytotch gradient computation feature\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "x=torch.tensor([1,2,3,4],dtype=torch.float32,requires_grad=False)\n",
    "y_gt=torch.tensor([2,4,6,8],dtype=torch.float32,requires_grad=False)\n",
    "\n",
    "w=torch.rand(1,requires_grad=True)\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "#model prediction\n",
    "def forward(x,w):\n",
    "    return x*w\n",
    "\n",
    "#loss function\n",
    "def loss(y_gt,y_pred):\n",
    "    return torch.mean((y_pred-y_gt)**2)\n",
    "\n",
    "#gradient computation\n",
    "def gradient(x,y_gt,y_pred):\n",
    "    return torch.mean(2*(y_pred-y_gt)*x)\n",
    "\n",
    "for i in range(100):\n",
    "    y_pred=forward(x,w)\n",
    "    l=loss(y_gt,y_pred)\n",
    "    l.backward()\n",
    "\n",
    "    # Gradient computation\n",
    "    grad=w.grad\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate*grad\n",
    "\n",
    "    w.grad.zero_() #to clear the gradient information\n",
    "    print(\"#########iteration##########\",i)\n",
    "    print(\"loss is\",l.item())\n",
    "    print(\"gradient is\",grad.item())\n",
    "    print(\"w is\",w.item())\n",
    "    print(\"predicted y is\",y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using autograd gradient computation\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32, requires_grad=False)\n",
    "y_gt = torch.tensor([2, 4, 6, 8], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "w = torch.rand(1, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model prediction\n",
    "def forward(x, w):\n",
    "    return x * w\n",
    "\n",
    "# Loss function (Mean Squared Error)\n",
    "def loss(y_gt, y_pred):\n",
    "    return torch.mean((y_pred - y_gt) ** 2)\n",
    "\n",
    "def update_weight(w: torch.Tensor, grad, learning_rate):\n",
    "    # not recommended\n",
    "    # Out of place operation\n",
    "    with torch.no_grad():\n",
    "        w = w - learning_rate * grad\n",
    "    \n",
    "    w.requires_grad_()  # Ensure the new tensor requires gradients\n",
    "    return w\n",
    "\n",
    "def update_weight_inplace(w: torch.Tensor, grad, learning_rate):\n",
    "    # recommended\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * grad\n",
    "\n",
    "# Training loop\n",
    "for i in range(100):\n",
    "    y_pred = forward(x, w)\n",
    "    l = loss(y_gt, y_pred)\n",
    "    l.backward()\n",
    "    \n",
    "    # Gradient\n",
    "    grad = w.grad\n",
    "    \n",
    "    # Update weight\n",
    "    w = update_weight(w, grad, learning_rate)\n",
    "    \n",
    "    # Clear the gradients if they exist\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    \n",
    "    # Logging\n",
    "    print(\"######### Iteration ##########\", i)\n",
    "    print(\"Loss:\", l.item())\n",
    "    print(\"Gradient:\", grad.item())\n",
    "    print(\"Updated weight:\", w.item())\n",
    "    print(\"Predicted y:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pytorch model,loss and optimizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32, requires_grad=False).view(-1, 1) #input shape is (4,1) (4 samples,1 feature)\n",
    "y_gt = torch.tensor([2, 4, 6, 8], dtype=torch.float32, requires_grad=False).view(-1, 1) #predicted shape is (4,1) (4 samples,1 output per sample)\n",
    "\n",
    "number_of_features=1\n",
    "number_of_outputs=1\n",
    "\n",
    "# to define single layer model\n",
    "model = nn.Linear(number_of_features,number_of_outputs) # a single layer linear model\n",
    "\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss() #mean squared error loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) #stochastic gradient descent optimizer\n",
    "\n",
    "# Training loop\n",
    "for i in range(100):\n",
    "    y_pred = model(x) #model prediction\n",
    "    l = criterion(y_pred, y_gt) #loss computation\n",
    "    \n",
    "    # Backward pass\n",
    "    l.backward() #Gradient computation\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step() #parameter update\n",
    "\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad() #clear the gradient information\n",
    "    \n",
    "    # Logging\n",
    "    print(\"######### Iteration ##########\", i)\n",
    "    print(\"Loss:\", l.item())\n",
    "    print(\"Updated weight:\", model.weight.item())\n",
    "    print(\"Updated bias:\", model.bias.item())\n",
    "    print(\"Predicted y:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build some simpler models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression & Logistic Regression\n",
    "\n",
    "An algorithm to determine the relationship between a dependent variable to multiple independent variables.\n",
    "\n",
    "Linear regression is applied when the value of dependent variable is continous\n",
    "while logistic regression is applied when the value of dependent variable is discrete.\n",
    "\n",
    "But yeah before applying a regression algorithm to a problem,followign condition should be met:\n",
    "The conditions that should be met before applying a regression algorithm to a problem are:\n",
    "\n",
    "1. Linearity: There should be a linear relationship between the independent variables and the dependent variable. This means that the change in the dependent variable should be proportional to the change in the independent variables.\n",
    "\n",
    "2. Independence: The independent variables should be independent of each other. There should not be any multicollinearity, which is a high correlation between independent variables.\n",
    "\n",
    "3. Homoscedasticity: The variance of the errors should be constant across all levels of the independent variables. This means that the spread of the residuals should be consistent.\n",
    "\n",
    "4. Normality: The errors should be normally distributed. This assumption is important for hypothesis testing and confidence intervals.\n",
    "\n",
    "5. No endogeneity: There should be no relationship between the errors and the independent variables. The errors should be random and not influenced by the independent variables.\n",
    "\n",
    "6. No autocorrelation: The errors should not be correlated with each other. This assumption is important when dealing with time series data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pytorch model,loss and optimizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "#load the linear regression data\n",
    "number_of_features=1 #number of features\n",
    "number_of_samples=100 #batch size\n",
    "number_of_outputs=1 \n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "\n",
    "y = y.view(y.shape[0], 1) #(batch_size,number_of_outputs)\n",
    "\n",
    "# to define single layer model\n",
    "model = nn.Linear(number_of_features,number_of_outputs) # a single layer linear model\n",
    "\n",
    "learning_rate = 0.1\n",
    "criterion = nn.MSELoss() #mean squared error loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) #stochastic gradient descent optimizer\n",
    "\n",
    "# Training loop\n",
    "for i in range(100):\n",
    "    y_pred = model(X) #model prediction\n",
    "    l = criterion(y_pred, y) #loss computation\n",
    "    \n",
    "    # Backward pass\n",
    "    l.backward() #Gradient computation\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step() #parameter update\n",
    "\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad() #clear the gradient information\n",
    "    \n",
    "    # Logging\n",
    "    if i%10==0:\n",
    "        print(\"######### Iteration ##########\", i)\n",
    "        print(\"Loss:\", l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistics regression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "number_of_features=2 #number of features\n",
    "number_of_samples=100 #batch size\n",
    "number_of_outputs=1\n",
    "\n",
    "#load the linear regression data\n",
    "X_numpy, y_numpy = datasets.make_classification(n_samples=100, n_features=number_of_features, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, random_state=4)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "\n",
    "y = y.view(y.shape[0], 1) #(batch_size,number_of_outputs)\n",
    "#sigmoid activation function\n",
    "sigmoid=nn.Sigmoid()\n",
    "\n",
    "model = nn.Linear(number_of_features,number_of_outputs) # a single layer linear model\n",
    "criterion = nn.BCELoss() #binary cross entropy loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) #stochastic gradient descent optimizer\n",
    "\n",
    "# Training loop\n",
    "for i in range(100):\n",
    "    y_pred = sigmoid(model(X)) #model prediction\n",
    "    l = criterion(y_pred, y) #loss computation\n",
    "    \n",
    "    # Backward pass\n",
    "    l.backward() #Gradient computation\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step() #parameter update\n",
    "\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad() #clear the gradient information\n",
    "    \n",
    "    # Logging\n",
    "    if i%10==0:\n",
    "        print(\"######### Iteration ##########\", i)\n",
    "        print(\"Loss:\", l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Here we will learn how to create custom class to load any data while training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_csv=\"/Users/sauravpandey/mygit_repos/DL-and-ML/learning-pytorch/datasets/wine.csv\"\n",
    "data=pd.read_csv(data_csv)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "from torch.utils.data import Dataset,DataLoader, Sampler\n",
    "from torch.utils.data.dataloader import _collate_fn_t, _worker_init_fn_t\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class WineDataLoader(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        dataset = np.loadtxt(data_csv, delimiter=\",\", skiprows=1, dtype=np.float32)\n",
    "        self.x = torch.from_numpy(dataset[:, 1:])\n",
    "        self.y = torch.from_numpy(dataset[:, 0]).view(-1,1)\n",
    "        self.n_samples = dataset.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "dataset=WineDataLoader()\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=4,shuffle=True,num_workers=0)\n",
    "\n",
    "#dummy trainig loop using dataloader\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(x,y) in enumerate(dataloader):\n",
    "        print(\"epoch\",epoch,\"batch\",i,\"x\",x,\"y\",y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "from torch.utils.data import Dataset,DataLoader, Sampler\n",
    "from torch.utils.data.dataloader import _collate_fn_t, _worker_init_fn_t\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "class WineDataLoader(Dataset):\n",
    "\n",
    "    def __init__(self,transforms:Optional[Callable[[Any],Any]]=None):\n",
    "        dataset = np.loadtxt(data_csv, delimiter=\",\", skiprows=1, dtype=np.float32)\n",
    "        self.x = dataset[:, 1:]\n",
    "        self.y = dataset[:, 0].reshape(-1,1)\n",
    "        self.n_samples = dataset.shape[0]\n",
    "\n",
    "        self.transforms=transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        inputs,labels=self.x[index], self.y[index]\n",
    "        if self.transforms:\n",
    "            inputs=self.transforms(inputs)\n",
    "        return inputs, labels\n",
    "    \n",
    "class ToTensor:\n",
    "    def __call__(self,sample):\n",
    "        return torch.tensor(sample)\n",
    "    \n",
    "class MutiplyByTwo:\n",
    "\n",
    "    def __init__(self,factor):\n",
    "        self.factor=factor\n",
    "\n",
    "    def __call__(self,sample):\n",
    "        return sample*self.factor\n",
    "    \n",
    "\n",
    "transforms=Compose([ToTensor(),MutiplyByTwo(factor=2)])\n",
    "dataset=WineDataLoader(transforms=transforms)\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=4,shuffle=True,num_workers=0)\n",
    "\n",
    "#dummy trainig loop using dataloader\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(x,y) in enumerate(dataloader):\n",
    "        print(\"epoch\",epoch,\"batch\",i,\"x\",x,\"y\",y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVOLUTIONAL NEURAL NETWORK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self,t):\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "    \n",
    "model=ConvNet()\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        images,labels=batch\n",
    "        preds=model(images)\n",
    "        l=loss(preds,labels)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(\"epoch\",epoch,\"loss\",l.item())\n",
    "\n",
    "#testing the model\n",
    "model.eval() #set the model in evaluation mode\n",
    "correct=0\n",
    "total=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images,labels=batch\n",
    "        preds=model(images)\n",
    "        _,predicted=torch.max(preds,dim=1)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(predicted==labels).sum().item()\n",
    "    print(\"accuracy\",correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "Training a already trained model to our own dataset.In simpler terms it is called fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## there are two ways to do the tansfer learning\n",
    "\n",
    "# 1 Fine tuning the whole model\n",
    "# base_model=BaseModel()\n",
    "# number_of_features=base_model.fc.in_features\n",
    "# base_model.fc=nn.Linear(number_of_features,number_of_classes)\n",
    "\n",
    "# 2 Freezing the base model and training only the classifier\n",
    "# base_model=BaseModel()\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad=False #freezing the base model\n",
    "# number_of_features=base_model.fc.in_features\n",
    "# base_model.fc=nn.Linear(number_of_features,number_of_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are two way to save the model (save the state dict or save the whole model)\n",
    "\n",
    "#saving the state dict(recommended,as it is more flexible)(only save the model weights)\n",
    "# but here to load the model you need to define the model architecture before loading the model\n",
    "\n",
    "#save the model\n",
    "torch.save(model.state_dict(),\"model.pth\")\n",
    "\n",
    "#load the model\n",
    "model=ConvNet()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval() #set the model in evaluation mode\n",
    "\n",
    "#saving the whole model(generally not recommended as it is less flexible)\n",
    "#save the model\n",
    "torch.save(model,\"model.pth\")\n",
    "\n",
    "#load the model\n",
    "model=torch.load(\"model.pth\")\n",
    "model.eval() #set the model in evaluation mode\n",
    "\n",
    "\n",
    "#saving the model with optimizer state(saving the checkpoint)\n",
    "#save the model\n",
    "torch.save({\n",
    "    \"epoch\":epoch,\n",
    "    \"model\":model.state_dict(),\n",
    "    \"optimizer\":optimizer.state_dict()\n",
    "},\"model.pth\")\n",
    "\n",
    "#loading the checkpoint\n",
    "checkpoint=torch.load(\"model.pth\")\n",
    "epoch=checkpoint[\"epoch\"]\n",
    "\n",
    "#define the model and optimizer\n",
    "model=ConvNet()\n",
    "optimizer= torch.optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "#load the model and optimizer state\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
